# -*- coding: utf-8 -*-
"""housing_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d2j5a-t23JbDs5uYQQOGraFa5qRVlykB
"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

import io
from google.colab import files
uploaded= files.upload()
df= pd.read_csv(io.StringIO(uploaded['housing.csv'].decode('utf-8')) , delim_whitespace = True , header = None)
df

dataset= df.values
x = dataset[:,0:13]
y= dataset[:,13]

#Create baseline Model
def baseline_model():
  model= Sequential()
  model.add(Dense(13 , activation='relu' , input_shape=(13,)))
  model.add(Dense(1))
  model.compile(optimizer='Adam' , loss= 'mse' , metrics=['mae'])
  return model

seed=7
np.random.seed(seed)

#run
estimator = KerasRegressor(build_fn = baseline_model , epochs = 100 , batch_size = 5 , verbose = 0 )
kfold = KFold(n_splits = 10 , random_state = seed)
results = cross_val_score(estimator , x , y , cv = kfold)
print("Results: %.2f (%.2f) MSE" % (abs(results.mean()), results.std()))

#Normalized using Standard Scaler
np.random.seed(seed)
estimators = []
estimators.append(('standardize' , StandardScaler()))
estimators.append(('mlp' , KerasRegressor(build_fn = baseline_model , epochs = 50 , batch_size = 5 , verbose = 0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits = 10 , random_state = seed)
results = cross_val_score(pipeline , x , y , cv = kfold)
print("Results: %2f(%2f)MSE"%(abs(results.mean()),results.std()))

#Evaluate a deeper network
def larger_model():
  model = Sequential()
  model.add(Dense(13 , activation='relu' , input_shape=(13,)))
  model.add(Dense(10 , activation='relu'))
  model.add(Dense(1))
  model.compile(optimizer='Adam' , loss='mse' , metrics=['mae'])
  return model

np.random.seed(seed)
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits=10, random_state=seed)
results = cross_val_score(pipeline, x, y, cv=kfold)
print("Larger: %.2f (%.2f) MSE" % (results.mean(), results.std()))

#Evaluate a wider Model
def wider_model():
  model= Sequential()
  model.add(Dense(13 , activation='relu' , input_shape=(13,)))
  model.add(Dense(20 , activation='relu'))
  model.add(Dense(1))
  model.compile(optimizer='Adam' , loss='mse' , metrics=['mae'])
  return model

np.random.seed(seed)
estimators = []
estimators.append(('standardize' , StandardScaler()))
estimators.append(('mlp' , KerasRegressor(build_fn = wider_model , epochs = 50 , batch_size = 5 , verbose = 0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits = 10 , random_state = seed)
results = cross_val_score(pipeline , x , y , cv = kfold)
print("Results: %2f(%2f)MSE"%(abs(results.mean()),results.std()))

#Overfit model
def overfit_model():
  model= Sequential()
  model.add(Dense(26 , activation='relu' , input_shape=(13,)))
  model.add(Dense(13 , activation='relu'))
  model.add(Dense(13 , activation='relu'))
  model.add(Dense(1))
  model.compile(optimizer= 'Adam' , loss='mse' , metrics=['mae'])
  return model

np.random.seed(seed)
estimators = []
estimators.append(('standardize' , StandardScaler()))
estimators.append(('mlp' , KerasRegressor(build_fn = overfit_model , epochs = 50 , batch_size = 5 , verbose = 0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits = 10 , random_state = seed)
results = cross_val_score(pipeline , x , y , cv = kfold)
print("Results: %2f(%2f)MSE"%(abs(results.mean()),results.std()))

#Tunned Model
from keras import regularizers
def tune_model():
  model= Sequential()
  model.add(Dense(13 , activation='relu' , input_shape=(13,)))
  model.add(Dense(6 , activation='relu'))
  model.add(Dense(1))
  model.compile(optimizer= 'Adam' , loss='mse' , metrics=['mae'])
  return model

#np.random.seed(seed)
estimators = []
estimators.append(('standardize' , StandardScaler()))
estimators.append(('mlp' , KerasRegressor(build_fn = tune_model , epochs = 50 , batch_size = 5 , verbose = 0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits = 10 , random_state = seed)
results = cross_val_score(pipeline , x , y , cv = kfold)
print("Results: %2f(%2f)MSE"%(abs(results.mean()),results.std()))

#Functional API
from keras.models import Model
from keras.layers import Input , Dense
from keras import regularizers
def functional_model():
  x= Input(shape=(13,))
  z1= Dense(13 ,  kernel_regularizer=regularizers.l2(0.001), activation='relu')(x)
  z2= Dense(6 ,  kernel_regularizer=regularizers.l2(0.001), activation='relu')(z1)
  y= Dense(1)(z2)
  model = Model(x , y)
  model.compile(optimizer= 'Adam' , loss='mse' , metrics=['mae'])
  return model

np.random.seed(seed)
estimators = []
estimators.append(('standardize' , StandardScaler()))
estimators.append(('mlp' , KerasRegressor(build_fn = functional_model , epochs = 50 , batch_size = 5 , verbose = 0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits = 10 , random_state = seed)
results = cross_val_score(pipeline , x , y , cv = kfold)
print("Results: %2f(%2f)MSE"%(abs(results.mean()),results.std()))

#Create subclassing
import tensorflow as tf
class Subclass(tf.keras.Model):
  def __init__(self):
    super(Subclass , self).__init__()
    self.dense1 = tf.keras.layers.Dense(13 , activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(6 , activation=tf.nn.relu)
    self.dense3 = tf.keras.layers.Dense(1)
  def call(self , inputs):
    a=self.dense1(x)
    a= self.dense2(a)
    return self.dense3(a)
def end():
  model= Subclass()
  model.compile(optimizer='adam' , loss='mse' , metrics=['mae'])
  return model

np.random.seed(seed)
estimators = []
estimators.append(('standardize' , StandardScaler()))
estimators.append(('mlp' , KerasRegressor(build_fn = end , epochs = 50 , batch_size = 5 , verbose = 0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits = 10 , random_state = seed)
results = cross_val_score(pipeline , x , y , cv = kfold)
print("Results: %2f(%2f)MSE"%(abs(results.mean()),results.std()))

#Build Model from scratch

complete_data = dataset.copy()
data = complete_data[:,0:13]
labels = complete_data[:,13]
train_data = data[:404]
train_labels = labels[:404]
test_data = data[404:]
test_labels = labels[404:]

mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data /= std

from keras import models
from keras import layers
def build_model():
  model = models.Sequential()
  model.add(layers.Dense(64, activation='relu',input_shape=(train_data.shape[1],)))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model

import numpy as np
k=4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []

for i in range(k):
  print('processing fold #', i)
  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = train_labels[i * num_val_samples: (i + 1) * num_val_samples]
  partial_train_data = np.concatenate(
      [train_data[:i * num_val_samples],train_data[(i + 1) * num_val_samples:]],axis=0)
  partial_train_targets = np.concatenate( [train_labels[:i * num_val_samples], train_labels[(i + 1) * num_val_samples:]],axis=0)
  model = build_model()
  model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores.append(val_mae)

all_scores

np.mean(all_scores)

model = build_model()
model.fit(train_data, train_labels,
epochs=80, batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(test_data, test_labels)
print(test_mse_score)

